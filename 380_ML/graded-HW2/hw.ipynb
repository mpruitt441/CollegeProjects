{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Worked with Marc Suda\n",
    "\n",
    "\n",
    "## HW 2\n",
    "\n",
    "In this homework you will implement your first learning model:  the perceptron.  \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"480\">\n",
    "\n",
    "If you haven't already, you should read the following sections from _Learning From Data_:\n",
    "\n",
    "* 1.1.2, for the definition of the perceptron\n",
    "* 1.4.1, for alternative error measures\n",
    "* 3.1.1, for the pocket algorithm\n",
    "* Page 21, for the definition of $E_{in}$\n",
    "\n",
    "These slides may also be useful: [slides](http://www.cs.rpi.edu/~magdon/courses/LFD-Slides/SlidesLect02.pdf)\n",
    "\n",
    "Your job is to follow the instructions given in the cells below.\n",
    "\n",
    "At the beginning and at some other places I will create some synthetic data for your perceptron to classify.\n",
    "\n",
    "**Due: 2/26**\n",
    "\n",
    "The collaboration policy is the same as HW1:  Up to three team members, with attribution.\n",
    "\n",
    "Do not copy code from the internet for any part unless the assignment specifically says that it's okay.\n",
    "\n",
    "---\n",
    "\n",
    "Notes:  For this assigment (and for all linear classifiers) it's important that you add a \"bias\" column of ones to $X$ and that the two possible classifications be $+1$ and $-1$.\n",
    "\n",
    "I've probably done this for you, just note where it happens and be careful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "## Producing synthetic data\n",
    "\"\"\"\n",
    "In this cell we create 35 points at random.\n",
    "We separate them with a line to create linearly separable data.\n",
    "The data is stored in the points array.\n",
    "Note that the first column of the points array is a \"bias\" column of all ones.\n",
    "The \"line _coef\" (coefficients) variable works like this:\n",
    "    if line_coef = (a,b,c)\n",
    "    then this describes the line\n",
    "    0 = a + bx + cy\n",
    "This line can be thought of as the intersection of the plane\n",
    "    z = a + bx + cy\n",
    "with the xy plane.\n",
    "Observe that in \n",
    "    0 = a + bx + cy\n",
    "there are really only two degrees of freedom, because we can divide everything by c to get\n",
    "\n",
    "    0 = A + Bx + y\n",
    "where A = a/c and B = b/c.\n",
    "(This assumes c != 0, but this assumption will almost always be true.)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 35 # number of points\n",
    "np.random.seed(42)  # So that everyone has the same \"random\" points\n",
    "points = np.zeros(3*N).reshape(N,3)\n",
    "points[:,1] = np.random.rand(N) #random x-coordinates\n",
    "points[:,2] = np.random.rand(N) #random y-coordinates\n",
    "points[:,0] = np.ones(N)        #bias column\n",
    "\n",
    "line_coef = (-1,0.9,1)          #this line will define whether a point is a positive or negative example\n",
    "pos = points.dot(line_coef) >= 0  # positive examples are above the line\n",
    "neg = ~pos   # negative examples are below the line\n",
    "\"\"\"\n",
    "Question:  do you understand what pos and neg are, and how they work?\n",
    "           you might want to print them out and experiment with them.\n",
    "           we use them to select out certain rows of a numpy array.\n",
    "           if this is confusing read the section here about \n",
    "           Boolean or \"mask\" index arrays:\n",
    "           https://docs.scipy.org/doc/numpy-1.10.0/user/basics.indexing.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "x = np.linspace(0,1)                # domain of decision boundary line\n",
    "y = -line_coef[1]*x - line_coef[0]  # this is the decision boundary line\n",
    "\n",
    "\"\"\"\n",
    "A note about colors... I used \"green\" for \"positive example\" and \"red\" for \"negative example\".\n",
    "If you happen to be red green colorblind (like 8% of all men) then please feel free to change the colors.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(1,figsize=(10,10))\n",
    "plt.scatter(points[pos,1],points[pos,2],c='green',marker=\"^\",s=100,label=\"positive example\")  #plot positive examples\n",
    "plt.scatter(points[neg,1],points[neg,2],c='red',marker=\"v\",s=100,label=\"negative example\")  #plot negative examples\n",
    "plt.plot(x,y,'b',label=\"decision boundary\")  #plot linear separator\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we want to \"forget\" the decision boundary and re-learn it from the data using the Perceptron Learning Algorithm (PLA).\n",
    "\n",
    "In the cell below we define `X` to be the points (along with bias column) and make `y` the corresponding classifications made by the decision boundary.\n",
    "\n",
    "Recall from the book that each `y` entry should be $\\pm 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "y = np.ones(N)\n",
    "y[neg] = -1\n",
    "y[pos] = 1\n",
    "X = points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 1\n",
    "\n",
    "Below is an outline of the PLA algorithm.\n",
    "\n",
    "Fill in the \"while loop\" part.\n",
    "\n",
    "The while loop should yield a weight vector `w` which makes the same decisions on X as the original decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Your code should work regardless of the dimensions of `X` and `y`.  Don't code in any magic numbers.\n",
    "\n",
    "If you find yourself writing a repeat loop in python (other than the given while loop) then you are doing something wrong.\n",
    "\n",
    "You should be using a matrix operation instead, like `X.dot(w)` or something.\n",
    "\n",
    "Use boolean indexing to find misclassified points.  \n",
    "\n",
    "Hint:  What is this?\n",
    "\n",
    "    np.argmax([False,True,False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "\n",
    "def PLA(X,y,iterations=2000):\n",
    "    \"\"\" \n",
    "    X: an Nx(d+1) matrix of datapoints with bias column\n",
    "    y: a Nx1 vector of classifications in {-1,1}\n",
    "    iterations: the maximum number of iterations\n",
    "    description:  Applies the perceptron learning algorithm to X,y for given number of iterations\n",
    "    returns: learned weight vector\n",
    "    \"\"\" \n",
    "    \n",
    "    assert(iterations >= 0)\n",
    "    w = np.random.rand(X.shape[1]) # random initial weights\n",
    "    _iterations = 0\n",
    "\n",
    "    while(_iterations < iterations):\n",
    "        #Part 1 worked with Marc Suda from class\n",
    "        A = np.sign(X.dot(w))!=y\n",
    "        if not (np.sign(X.dot(w))==y).all():\n",
    "            #if a x gets misclassified, update weights\n",
    "            t = np.argmax(A)\n",
    "            w= w + y[t]*X[t]\n",
    "            _iterations += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(f\"Done in {_iterations} iterations\")\n",
    "    print(np.sign(X.dot(w)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "w = PLA(X,y,2000)\n",
    "wp = w/w[-1]  # dividing out c as described in comments to first code cell\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Checking your work\n",
    "\n",
    "The following code will plot your decision boundary together with the original decision boundary.\n",
    "\n",
    "The result should look something like this:\n",
    "\n",
    "![img](wtf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "line_coef = (-1,0.9,1)\n",
    "pos = points.dot(line_coef) >= 0\n",
    "neg = points.dot(line_coef) < 0\n",
    "\n",
    "\n",
    "x = np.linspace(0,1)\n",
    "yy = -line_coef[1]*x - line_coef[0]  \n",
    "yh = -wp[1]*x - wp[0]\n",
    "\n",
    "plt.figure(1,figsize=(10,10))\n",
    "plt.title(\"Learned decision boundary together with original decision boundary\")\n",
    "\n",
    "plt.scatter(points[pos,1],points[pos,2],c='green',s=100,label=\"positive example\")  #plot positive examples\n",
    "plt.scatter(points[neg,1],points[neg,2],c='red',s=100,label=\"negative example\")  #plot negative examples\n",
    "plt.plot(x,yy,'b',label=\"decision boundary\")  #plot linear separator\n",
    "plt.plot(x,yh,'brown',label=\"learned boundary\")  #plot linear separator\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Non linearly-separable data\n",
    "\n",
    "The way we used the perceptron to learn the decision boundary was artificially easy.\n",
    "\n",
    "It's almost never the case that real data can actually be classified perfectly using a line as a decision boundary.  \n",
    "\n",
    "In this part of the assignment we will implement the Pocket Algorithm to learn a decision boundary that does a good job of classifying data that is not quite linearly separable.  \n",
    "\n",
    "The pocket algorithm is described on pg. 80 of the _Learning From Data_ book.\n",
    "\n",
    "First we make the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "N = 100 # number of points\n",
    "np.random.seed(42)  # So that everyone has the same \"random\" points\n",
    "points = np.zeros(3*N).reshape(N,3)\n",
    "points[:,1] = np.random.rand(N) #random x-coordinates\n",
    "points[:,2] = np.random.rand(N) #random y-coordinates\n",
    "points[:,0] = np.ones(N)        #bias column\n",
    "\n",
    "line_coef = (-1,0.9,1)          #this line will define whether a point is a positive or negative example\n",
    "pos = points.dot(line_coef) >= 0  # positive examples are above the line\n",
    "\n",
    "for i in range(N//10):  # Now we flip about 10% of the data (could be less b/c collisions)\n",
    "    r = np.random.randint(len(pos))\n",
    "    pos[r] = ~pos[r]    # Flip the truth condition\n",
    "    \n",
    "neg = ~pos   # negative examples are below the line (except for flipped ones)\n",
    "\n",
    "x = np.linspace(0,1)                # domain of decision boundary line\n",
    "y = -line_coef[1]*x - line_coef[0]  # this is the decision boundary line\n",
    "\n",
    "plt.figure(1,figsize=(10,10))\n",
    "plt.scatter(points[pos,1],points[pos,2],c='green',marker=\"^\",s=100,label=\"positive example\")  #plot positive examples\n",
    "plt.scatter(points[neg,1],points[neg,2],c='red',s=100,marker=\"v\",label=\"negative example\")  #plot negative examples\n",
    "#plt.plot(x,y,'b',label=\"decision boundary\")  #plot linear separator\n",
    "plt.legend()\n",
    "plt.title(\"Non-linearly separable data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Parts 2 and 3\n",
    "\n",
    "In the code below please fill in the missing part of the pocket algorithm.\n",
    "\n",
    "Also implement the `E_in` function (see p. 21 and p. 80).\n",
    "\n",
    "Hint:  What is this?\n",
    "\n",
    "    np.mean([True,False,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "y = np.ones(N)\n",
    "y[neg] = -1\n",
    "y[pos] = 1\n",
    "X = points\n",
    "\n",
    "def E_in(X,w,y):\n",
    "    k = np.sign(X.dot(w))!=y #returns true or false\n",
    "    e = (1/len(X)) * np.mean(k)\n",
    "    return e\n",
    "\n",
    "def Pocket_Algorithm(X,y,iterations=2000,E_in=E_in):\n",
    "    \n",
    "    \n",
    "    assert(iterations >= 0)\n",
    "    w = np.random.rand(X.shape[1]) # random initial weights\n",
    "    least_error = E_in(X,w,y)\n",
    "    w_hat = np.copy(w)  ## Be sure to make a deep copy\n",
    "    _iterations = 0\n",
    "    while(_iterations < iterations):\n",
    "        _iterations += 1\n",
    "        # Part 2\n",
    "        # run PLA ONCE to obtain w(t+1)\n",
    "        A = np.sign(X.dot(w))!=y\n",
    "        t = np.argmax(A)\n",
    "        w= w + y[t]*X[t]\n",
    "        Ebest = E_in(X,w_hat,y)\n",
    "        Enew = E_in(X,w,y)\n",
    "        #w= w + Enew*y[t]*X[t] # took this line from: https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "        #edited it from my PLA algorithm to multiply by the Enew\n",
    "        if Enew < Ebest:\n",
    "            w_hat = np.copy(w)\n",
    "        if (np.sign(X.dot(w))==y).all():\n",
    "            break\n",
    "    print(w_hat)\n",
    "    print(A)\n",
    "    print(np.argmax(A))\n",
    "    print(f\"Done in {_iterations} iterations\")\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "w = Pocket_Algorithm(X,y,2000)\n",
    "wp = w/w[-1]  # dividing out c as described in comments to first code cell\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Checking your work\n",
    "\n",
    "You can use the code below to check the correctess of your implementation.\n",
    "\n",
    "The result should be an image that looks roughly like this:\n",
    "    \n",
    "![img](non_lin.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#### line_coef = (-1,0.9,1)\n",
    "\n",
    "\n",
    "x = np.linspace(0,1)\n",
    "yy = -line_coef[1]*x - line_coef[0]  \n",
    "yh = -wp[1]*x - wp[0]\n",
    "\n",
    "plt.figure(1,figsize=(10,10))\n",
    "plt.title(\"Learned decision boundary together with original decision boundary\")\n",
    "\n",
    "plt.scatter(points[pos,1],points[pos,2],c='green',s=100,label=\"positive example\")  #plot positive examples\n",
    "plt.scatter(points[neg,1],points[neg,2],c='red',s=100,label=\"negative example\")  #plot negative examples\n",
    "plt.plot(x,yy,'b',label=\"decision boundary\")  #plot linear separator\n",
    "plt.plot(x,yh,'brown',label=\"learned boundary\")  #plot linear separator\n",
    "plt.axis([np.min(points[:,1]),np.max(points[:,1]),np.min(points[:,2]),np.max(points[:,2])])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Scale\n",
    "\n",
    "How well does the perceptron scale?  We used it on 35 points.  Could we use it on a million?\n",
    "\n",
    "What about 35 points that are each million dimensional?\n",
    "\n",
    "The following code plots the way in which PLA convergence time changes as the number of points increases.\n",
    "\n",
    "We stick with 3d points.\n",
    "\n",
    "On average the convergence rate seems okay, but if you comment out this line:\n",
    "\n",
    "    plt.axis([0,3500,0,100000])\n",
    "\n",
    "you see that one or two datasets take a very long time to fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_data(N,line_coef=(-1,0.9,1),seed=42):\n",
    "    np.random.seed(seed)  # So that everyone has the same \"random\" points\n",
    "    points = np.zeros(3*N).reshape(N,3)\n",
    "    points[:,1] = np.random.rand(N) #random x-coordinates\n",
    "    points[:,2] = np.random.rand(N) #random y-coordinates\n",
    "    points[:,0] = np.ones(N)        #bias column\n",
    "\n",
    "    line_coef = (-1,0.9,1)          #this line will define whether a point is a positive or negative example\n",
    "    pos = points.dot(line_coef) >= 0  # positive examples are above the line\n",
    "    return points,pos\n",
    "\n",
    "## This is just like PLA, but also returns the number of iterations\n",
    "def iter_PLA(X,y,iterations=2000):\n",
    "    \"\"\" \n",
    "    X: an Nx(d+1) matrix of datapoints\n",
    "    y: a Nx1 vector of classifications in {-1,1}\n",
    "    iterations: the maximum number of iterations\n",
    "    description:  Applies the perceptron learning algorithm to X,y for given number of iterations\n",
    "    returns: learned weight vector and number of iterations\n",
    "    \"\"\" \n",
    "    \n",
    "    assert(iterations >= 0)\n",
    "    w = np.random.rand(X.shape[1]) # random initial weights\n",
    "    _iterations = 0\n",
    "    while(_iterations < iterations):\n",
    "        A = np.sign(X.dot(w))!=y\n",
    "        if not (np.sign(X.dot(w))==y).all():\n",
    "            #if a x gets misclassified, update weights\n",
    "            t = np.argmax(A)\n",
    "            w= w + y[t]*X[t]\n",
    "            _iterations += 1\n",
    "        else:\n",
    "            break\n",
    "        pass # something must go here to avoid syntax error\n",
    "    return w,_iterations\n",
    "\n",
    "## _N and _I were produced from this code\n",
    "## This takes a long time to run\n",
    "## Don't run it -- it's just included for completeness\n",
    "## But if you do run it, be sure to fill in the missing part of iter_PLA.\n",
    "\"\"\"\n",
    "k = 100\n",
    "_N = np.arange(35,35*k+1,35)\n",
    "_I = np.zeros(k)\n",
    "for i,n in enumerate(_N):\n",
    "    X,pos = get_data(n)\n",
    "    y = 2*pos -1\n",
    "    w,_I[i] = iter_PLA(X,y,1000000)\n",
    "\"\"\"\n",
    "\n",
    "##  Loading arrays from saved files\n",
    "_N = np.load(\"_N.npy\")\n",
    "_I = np.load(\"_I.npy\")\n",
    "\n",
    "plt.axis([0,10000,0,100000])\n",
    "plt.scatter(_N,_I)\n",
    "plt.xlabel(\"Data size\")\n",
    "plt.ylabel(\"Iterations until convergence\")\n",
    "plt.title(\"PLA convergence rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I changed the above to show the convergence as the data size reaches 10,000 points. It seems that after 2000 data points, PLA starts to take longer and longer. Running it on a million points is infeasible. If you do, you might want to wait a couple days before checking back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pocket scaling\n",
    "\n",
    "Below we examine how the pocket algorithm `E_in` quantity varies if we fix the number of iterations at 2000 (the default) and increase the number of points it must classify.\n",
    "\n",
    "The best possible performance is about 10% error, because 10% of the points are classified incorrectly by an ideal linear separation.  \n",
    "\n",
    "In the graph it looks as though things might be improving for bigger datasets (error is going down at around $N=1000$).\n",
    "\n",
    "Is this real?  \n",
    "\n",
    "### Part 4\n",
    "Confirm or disconfirm the idea that pocket accuracy increases with $N$ by extending the plot to have a domain from 0 to 1500.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def get_data_nonlin(N,line_coef=(-1,0.9,1),seed=42):\n",
    "    np.random.seed(seed)  # So that everyone has the same \"random\" points\n",
    "    points = np.zeros(3*N).reshape(N,3)\n",
    "    points[:,1] = np.random.rand(N) #random x-coordinates\n",
    "    points[:,2] = np.random.rand(N) #random y-coordinates\n",
    "    points[:,0] = np.ones(N)        #bias column\n",
    "\n",
    "    line_coef = (-1,0.9,1)          #this line will define whether a point is a positive or negative example\n",
    "    pos = points.dot(line_coef) >= 0  # positive examples are above the line\n",
    "    for i in range(N//10):  # Now we flip about 10% of the data (could be less b/c collisions)\n",
    "        r = np.random.randint(len(pos))\n",
    "        pos[r] = ~pos[r]\n",
    "    return points,pos\n",
    "\n",
    "_N = np.arange(1,1501,1)\n",
    "_L = np.zeros(1501-1)\n",
    "for i,n in enumerate(_N):\n",
    "    X,pos = get_data_nonlin(n)\n",
    "    y = 2*pos -1\n",
    "    w=Pocket_Algorithm(X,y)\n",
    "    _L[i] = E_in(X,w,y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.scatter(_N,_L,alpha=0.2)\n",
    "plt.xlabel(\"Data size\")\n",
    "plt.ylabel(r\"Pocket $E_{in}$\")\n",
    "plt.title(\"Pocket algorithm performance (2000 iterations)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After changing the boundary to 1500, the line looks similar as if the boundary was 1000. Therefore, there is no valid proof here that pocket accuracy increases as N increases.Looking at the data, it seems to do best when N is less than 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Real data\n",
    "\n",
    "We now examine how the pocket algorithm performs on a real dataset.\n",
    "\n",
    "The dataset we use is described here: \n",
    "    \n",
    "[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))\n",
    "\n",
    "You should read the [dataset description](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names).  \n",
    "\n",
    "For the first exercise we reduce the problem to two dimensions by considering only columns 8 and 20 of the dataset.\n",
    "\n",
    "This throws away information and will reduce accuracy, but it has the advantage of letting us see what's happening in a 2d graph.\n",
    "\n",
    "### Part 5\n",
    "\n",
    "What are the meanings of columns 8 and 20 according to the dataset description?\n",
    "\n",
    "Update the image produced below to have the relevant information on the relevant axes.  \n",
    "\n",
    "Use `xlabel` and `ylabel` as in the above plot.  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Real data\n",
    "D = np.loadtxt(open(\"num_data.txt\", \"rb\"), delimiter=\",\", skiprows=0)\n",
    "\n",
    "\n",
    "x  = D[:,8]\n",
    "y  = D[:,20]\n",
    "\n",
    "malignant = D[:,1]==1  ## Column 1 represents benign (0) or malignant (1)\n",
    "xm = x[malignant]\n",
    "ym = y[malignant]\n",
    "\n",
    "benign = D[:,1]==0\n",
    "xb = x[benign]\n",
    "yb = y[benign]\n",
    "\n",
    "plt.scatter(xm,ym,label=\"malignant\",alpha=0.3)\n",
    "plt.scatter(xb,yb,label=\"benign\",alpha=0.3)\n",
    "plt.xlabel(\"Concativity\")\n",
    "plt.ylabel(\"Symmetry\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 6\n",
    "\n",
    "Below we use the pocket algorithm to find a linear separator `w` for the breast cancer data and then output its `E_in`.\n",
    "\n",
    "What is the accuracy?  That is, what percentage of the datapoints does the classifier `w` classify correctly?\n",
    "\n",
    "Change the code below so that the accuracy is printed as well as `E_in`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = np.ones(D.shape[0]*3).reshape(D.shape[0],3)\n",
    "X[:,1] = D[:,8]\n",
    "X[:,2] = D[:,20]\n",
    "\n",
    "y = 2*malignant -1  # so that y in {-1,1}, not {0,1}\n",
    "\n",
    "\n",
    "w = Pocket_Algorithm(X,y,2000)\n",
    "wp = w/w[-1]  # dividing out c as described in comments to first code cell\n",
    "\n",
    "\n",
    "Ething = E_in(X,w,y)\n",
    "acc = (1 - Ething) * 100\n",
    "print(f\"Ein = {Ething} and Accuracy = {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the plot below we visualize the learned decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "plt.scatter(xm,ym,label=\"malignant\",alpha=0.3)\n",
    "plt.scatter(xb,yb,label=\"benign\",alpha=0.3)\n",
    "\n",
    "yh = -wp[1]*X[:,1] - wp[0]\n",
    "plt.plot(x,yh,'brown',label=\"learned boundary\")  #plot linear separator\n",
    "plt.axis([-0.05,0.5,0,.08])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Alternative error measures\n",
    "\n",
    "The pocket algorithm uses so-called \"hinge loss\" meaning a correct classification has a \"loss\" of 0 and an incorrect classification has a \"loss\" of 1.\n",
    "Then `E_in` is the average loss.\n",
    "\n",
    "But in reality different kinds of error incur different costs.  \n",
    "\n",
    "An error may result from either a false positive, or a false negative.\n",
    "\n",
    "## Part 7\n",
    "\n",
    "For the cancer data should false positives and false negatives be weighted equally?\n",
    "\n",
    "Why or why not?  What happens to the patient in the case of a false negative?\n",
    "\n",
    "What happens in the case of a false positive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "No, false positives and false negatives should NOT be weighted equally.\n",
    "\n",
    "A false positive isn't as drastic as a false negative. Someone who is a false positive might be scared that they actually have cancer, but when more scientific analysis is done on them, they won't have it. The worst case scenario here for a false positive is a scare.\n",
    "\n",
    "Someone who is a false negative will believe they're okay, but then they'll just get worse and worse. Eventually, it might be too late for them. The worst case scenario here is death.\n",
    "\n",
    "Therefore, false negatives should be weighed more, such as 100.\n",
    "False positives should be weight as something like 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "The following two plots show what happens when we use a different error measure `E_cancer` (described below) with the pocket algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 8\n",
    "\n",
    "Complete the `E_cancer` function by adding a return statement.\n",
    "\n",
    "The function should return average loss where:\n",
    "\n",
    "1. the loss resulting from a correct classification is 0\n",
    "2. the loss resulting from a false negative is 10\n",
    "3. the loss resulting from a false positive is 1\n",
    "\n",
    "### Part 9 \n",
    "\n",
    "Suppose $w_{hinge}$ is the boundary learned by `Pocket_Algorithm(X,y)` and $w_{cancer}$ is the boundary learned by `Pocket_Algorithm(X,y,E_in=E_cancer)` (see below).\n",
    "\n",
    "Accuracy is the percentage of sample points classified correctly.\n",
    "\n",
    "What is the accuracy of the boundary $w_{hinge}$? \n",
    "\n",
    "What is the accuracy of $w_{cancer}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def E_cancer(X,w,y):\n",
    "    \"\"\"\n",
    "    false negatives are 10 times worse than false positives\n",
    "    \"\"\"\n",
    "    misclassified = (np.sign(X.dot(w)) != y)\n",
    "    malignant = (y==1)\n",
    "    false_negative = misclassified & malignant\n",
    "    false_positive = misclassified & (~malignant)\n",
    "    wee = [0,10,1]\n",
    "    \n",
    "    \n",
    "    return X, wee,(1/len(X)) * np.mean(misclassified)\n",
    "\n",
    "\n",
    "x  = D[:,8]\n",
    "y  = D[:,20]\n",
    "\n",
    "malignant = D[:,1]==1\n",
    "xm = x[malignant]\n",
    "ym = y[malignant]\n",
    "\n",
    "benign = D[:,1]==0\n",
    "xb = x[benign]\n",
    "yb = y[benign]\n",
    "\n",
    "X = np.ones(D.shape[0]*3).reshape(D.shape[0],3)\n",
    "X[:,1] = D[:,8]\n",
    "X[:,2] = D[:,20]\n",
    "\n",
    "y = 2*malignant -1\n",
    "\n",
    "w = Pocket_Algorithm(X,y,E_in = E_cancer)\n",
    "wp = w/w[-1]  # dividing out c as described in comments to first code cell\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(2,2,figsize=(12,12))\n",
    "fig.suptitle(\"Pocket with E_in = E_cancer\")\n",
    "axes[0,0].scatter(xm,ym,label=\"malignant\",alpha=0.3)\n",
    "axes[0,0].scatter(xb,yb,label=\"benign\",alpha=0.3)\n",
    "\n",
    "yh = -wp[1]*X[:,1] - wp[0]\n",
    "axes[0,0].plot(x,yh,'brown',label=\"learned boundary\")  #plot linear separator\n",
    "axes[0,0].axis([-0.05,0.5,0,.08])\n",
    "axes[0,0].legend()\n",
    "\n",
    "misclassified = (np.sign(X.dot(w)) != y)\n",
    "malignant = (y==1)\n",
    "false_negative = misclassified*malignant\n",
    "false_positive = misclassified*(~malignant)\n",
    "\n",
    "xfn = X[false_negative][:,1]\n",
    "yfn = X[false_negative][:,2]\n",
    "xb = X[~false_negative][:,1]\n",
    "yb = X[~false_negative][:,2]\n",
    "axes[0,1].scatter(xfn,yfn,label=\"false_negative\",alpha=0.3,color='red')\n",
    "axes[0,1].scatter(xb,yb,label=\"not false negative\",alpha=0.3,color='green')\n",
    "axes[0,1].legend()\n",
    "\n",
    "classified_mal = np.sign(X.dot(w))==1\n",
    "x_mal = X[classified_mal][:,1]\n",
    "y_mal = X[classified_mal][:,2]\n",
    "\n",
    "x_ben = X[~classified_mal][:,1]\n",
    "y_ben = X[~classified_mal][:,2]\n",
    "\n",
    "axes[1,0].scatter(x_mal,y_mal,label=\"classified malignant\",alpha=0.3,color='red')\n",
    "axes[1,0].scatter(x_ben,y_ben,label=\"classified benign\",alpha=0.3,color='green')\n",
    "axes[1,0].legend()\n",
    "\n",
    "x_mcl = X[misclassified][:,1]\n",
    "y_mcl = X[misclassified][:,2]\n",
    "\n",
    "x_ccl = X[~misclassified][:,1]\n",
    "y_ccl = X[~misclassified][:,2]\n",
    "\n",
    "axes[1,1].scatter(x_mcl,y_mcl,label=\"misclassifed\",alpha=0.3,color='red')\n",
    "axes[1,1].scatter(x_ccl,y_ccl,label=\"correctly classified\",alpha=0.3,color='green')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print((1- E_cancer)*100)\n",
    "#print((1-E_in) * 100)\n",
    "#wcancer will be way more accurate than whinge because wcancer's weights are more punishing, meaning that they assign how much false negatives and positives should weigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "wcancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "x  = D[:,8]\n",
    "y  = D[:,20]\n",
    "\n",
    "malignant = D[:,1]==1\n",
    "xm = x[malignant]\n",
    "ym = y[malignant]\n",
    "\n",
    "benign = D[:,1]==0\n",
    "xb = x[benign]\n",
    "yb = y[benign]\n",
    "\n",
    "X = np.ones(D.shape[0]*3).reshape(D.shape[0],3)\n",
    "X[:,1] = D[:,8]\n",
    "X[:,2] = D[:,20]\n",
    "\n",
    "y = 2*malignant -1\n",
    "\n",
    "w = Pocket_Algorithm(X,y)\n",
    "                    \n",
    "wp = w/w[-1]  # dividing out c as described in comments to first code cell\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(2,2,figsize=(12,12))\n",
    "fig.suptitle(\"Pocket with E_in = Hinge loss\")                       \n",
    "axes[0,0].scatter(xm,ym,label=\"malignant\",alpha=0.3)\n",
    "axes[0,0].scatter(xb,yb,label=\"benign\",alpha=0.3)\n",
    "\n",
    "yh = -wp[1]*X[:,1] - wp[0]\n",
    "axes[0,0].plot(x,yh,'brown',label=\"learned boundary\")  #plot linear separator\n",
    "axes[0,0].axis([-0.05,0.5,0,.08])\n",
    "axes[0,0].legend()\n",
    "\n",
    "misclassified = (np.sign(X.dot(w)) != y)\n",
    "malignant = (y==1)\n",
    "false_negative = misclassified*malignant\n",
    "false_positive = misclassified*(~malignant)\n",
    "\n",
    "xfn = X[false_negative][:,1]\n",
    "yfn = X[false_negative][:,2]\n",
    "xb = X[~false_negative][:,1]\n",
    "yb = X[~false_negative][:,2]\n",
    "axes[0,1].scatter(xfn,yfn,label=\"false_negative\",alpha=0.3,color='red')\n",
    "axes[0,1].scatter(xb,yb,label=\"not false negative\",alpha=0.3,color='green')\n",
    "axes[0,1].legend()\n",
    "\n",
    "classified_mal = np.sign(X.dot(w))==1\n",
    "x_mal = X[classified_mal][:,1]\n",
    "y_mal = X[classified_mal][:,2]\n",
    "\n",
    "x_ben = X[~classified_mal][:,1]\n",
    "y_ben = X[~classified_mal][:,2]\n",
    "\n",
    "axes[1,0].scatter(x_mal,y_mal,label=\"classified malignant\",alpha=0.3,color='red')\n",
    "axes[1,0].scatter(x_ben,y_ben,label=\"classified benign\",alpha=0.3,color='green')\n",
    "axes[1,0].legend()\n",
    "\n",
    "x_mcl = X[misclassified][:,1]\n",
    "y_mcl = X[misclassified][:,2]\n",
    "\n",
    "x_ccl = X[~misclassified][:,1]\n",
    "y_ccl = X[~misclassified][:,2]\n",
    "\n",
    "axes[1,1].scatter(x_mcl,y_mcl,label=\"misclassifed\",alpha=0.3,color='red')\n",
    "axes[1,1].scatter(x_ccl,y_ccl,label=\"correctly classified\",alpha=0.3,color='green')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Full dimensionality\n",
    "\n",
    "How much better is the pocket algorithm when we use the full dataset, rather than two columns?\n",
    "\n",
    "If the following breaks your code then your `Pocket_Algorithm` has problems (assumptions about dimensionality?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = np.ones(D.shape[0]*(D.shape[1]-1)).reshape(D.shape[0],D.shape[1]-1)\n",
    "X[:,1:] = D[:,2:]\n",
    "y = 2*D[:,1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "w = Pocket_Algorithm(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "E_in(X,w,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below is the linear boundary found by linear classification (to be discussed soon).\n",
    "\n",
    "Note that it's faster and better than the perceptron (sorry perceptron -- you're still useful for building neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "w = np.linalg.pinv(X).dot(y)\n",
    "E_in(X,w,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The pocket algorithm performs way better when using all of the columns to make a decision. The in-sample error is lower using the Pocket Algorithm with all the data while using two columns is higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}